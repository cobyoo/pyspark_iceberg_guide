{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c31937-39c3-4c2f-ba84-54e63c8bd0b0",
   "metadata": {},
   "source": [
    "# Purpose of Document Writing\n",
    "- This document is written to guide AI engineers on how to use Apache Spark and Apache Iceberg in a Python environment. \n",
    "- The main purpose is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac8795-7fd2-4604-b1b9-95115fa6b70a",
   "metadata": {},
   "source": [
    "#### Understanding the Technology\n",
    "- Apache Spark: A distributed processing framework designed for large-scale data processing and analysis, easily accessible through its Python API (pyspark). \n",
    "- Apache Iceberg: A data lake table format that supports schema evolution and ACID transactions, which enhances data management and analysis efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa98be3-8c82-46fe-aa0b-7ca5b2c201d6",
   "metadata": {},
   "source": [
    "#### Efficient Data Processing\n",
    "- Performance Optimization: Suggests methods to optimize Spark performance when handling large datasets and how to use Iceberg to enhance data storage and query performance.\n",
    "- Maintaining Data Integrity: Describes how to maintain data integrity using Iceberg's ACID transactions and how to leverage schema evolution features to manage data models flexibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3b372-f648-426a-b746-5266926b4914",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0dd2d-7ef8-449a-bea7-033a8d238e06",
   "metadata": {},
   "source": [
    "# Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293d17e-90e0-4b72-a4ec-01cc11dff2b1",
   "metadata": {},
   "source": [
    "TIP : The Apache Spark version is 3.5.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9fa42-307d-48bc-9234-0b1d5af5448a",
   "metadata": {},
   "source": [
    "#### [What is Apache Spark?](https://spark.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880e16a-5ff9-435f-a99a-879d1627e95a",
   "metadata": {},
   "source": [
    "- Apache Spark is an open-source cluster computing framework for large-scale data processing.\n",
    "- It supports fast data processing through in-memory operations and is widely used as a platform to implement various data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2764dda-5a96-404a-8c61-67d92245e4d9",
   "metadata": {},
   "source": [
    "#### __Contents__ \n",
    "- DataFrame\n",
    "- Basic Operations on DataFrame\n",
    "- Aggregate Operations\n",
    "- SQL\n",
    "- Reading a JSON File into a DataFrame in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3ba23-0570-4ec6-90a6-cedba77af52b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99127463-581d-4db5-97c1-09b9223b3049",
   "metadata": {},
   "source": [
    "#### __DataFrame__ \n",
    "This section shows how to create a Spark DataFrame and run simple tasks with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a3b0c3-0254-4076-866d-8b6a95b11d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 05:13:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession # Which is needed to start processing data in PySpark\n",
    "\n",
    "# Prepares and creates a new SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Elice Spark Example\") \\\n",
    "    .getOrCreate() # Creates a new SparkSession based on the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50278ad-28e1-44cb-84e6-d09207ab128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = [(\"Rabbit\", 34), (\"Elice\", 45), (\"Helpy\", 29)]\n",
    "\n",
    "# Column Name\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a002d46-6d12-43e4-b5d2-ebbe376f7fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Rabbit| 34|\n",
      "| Elice| 45|\n",
      "| Helpy| 29|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the contents of a DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08f4a8-5ff8-4da9-9633-030e7f9a6b10",
   "metadata": {},
   "source": [
    "#### __Basic Operations on DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff1f211-0ed0-4016-8feb-716229b5cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Rabbit| 34|\n",
      "| Elice| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter only people who are 30 years old or older\n",
    "df_filtered = df.filter(df.Age > 30)\n",
    "\n",
    "# Filtered Data Output\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881376d8-acdf-4b45-91fd-2bb5310d6bd8",
   "metadata": {},
   "source": [
    "#### __Aggregate Operations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48efe3e-baa2-45c6-94dc-ead50cc7fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(Age)|\n",
      "+--------+\n",
      "|    36.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Average Age\n",
    "df_avg_age = df.groupBy().avg(\"Age\")\n",
    "\n",
    "# Print average age\n",
    "df_avg_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736deb64-7fd6-4e84-bff9-16099d753a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Rabbit|    34.0|\n",
      "| Elice|    45.0|\n",
      "| Helpy|    29.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How to Calculate Average Age Using SQL \n",
    "spark.sql(\"select name, avg(age) from {df} group by name\", df=df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de094dd-284f-4082-b6c1-24a345301c65",
   "metadata": {},
   "source": [
    "#### __SQL__\n",
    "Spark module that can run SQL queries on structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99da11f-39ed-4d57-98f8-2e7a17a4589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Parquet table for easy access\n",
    "df.write.mode('overwrite').saveAsTable(\"elice_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbf9db9-3f5d-4454-9618-176ae38fc72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Rabbit| 34|\n",
      "| Elice| 45|\n",
      "| Helpy| 29|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check if we can access the table using the table name\n",
    "spark.sql(\"select * from elice_people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335a7211-5d25-4476-8814-98e06158586b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use SQL to insert some data rows into the table\n",
    "spark.sql(\"INSERT INTO elice_people VALUES ('AI', 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ced11dd-7eae-4337-82d7-d1227787b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Rabbit| 34|\n",
      "| Elice| 45|\n",
      "| Helpy| 29|\n",
      "|    AI|  4|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To check if a row has been inserted, let's look at the table contents\n",
    "spark.sql(\"select * from elice_people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804fc440-2947-4aec-969f-0ee743966af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Rabbit| 34|\n",
      "| Elice| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only show people who are 30 years old or older\n",
    "df = spark.sql(\"SELECT * FROM elice_people WHERE Age >= 30\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6f7d16-7586-4874-b6ef-85c3a2ba0092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|AverageAge|\n",
      "+----------+\n",
      "|      28.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating Average Age\n",
    "df = spark.sql(\"SELECT AVG(Age) AS AverageAge FROM elice_people\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc21e4-3aae-4c22-afaa-f068e1c88695",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292c8a0-ed79-40a6-8a4e-cb8868f94d71",
   "metadata": {},
   "source": [
    "#### __Supported file formats__\n",
    "Apache Spark, It supports various file formats, providing flexibility to read and write data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcfce1f-2311-4af7-8b3e-44fb60fbeb34",
   "metadata": {},
   "source": [
    "* __TXT Files (.txt)__\n",
    "``` python\n",
    "df = spark.read.text(\"home/elicer/workload/file.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70965c-2de7-412c-b39e-178aae799ff3",
   "metadata": {},
   "source": [
    "* __CSV Files (.csv)__\n",
    "``` python\n",
    "df = spark.read.csv(\"home/elicer/workload/file.csv\", header=True, inferSchema=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8737b-e857-492f-8948-df0abc612404",
   "metadata": {},
   "source": [
    "* __JSON Files (.json)__\n",
    "``` python\n",
    "df = spark.read.json(\"home/elicer/workload/file.json\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b731d-e3a3-4ed2-944a-985ad1ff3f8a",
   "metadata": {},
   "source": [
    "* __Parquet (.parquet)__\n",
    "``` python\n",
    "df = spark.read.parquet(\"home/elicer/workload/file.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e1dc3-a8a6-4eb3-ac96-5520ef2d837f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3d88e-4ca2-4383-9bb5-5316f8dbf562",
   "metadata": {},
   "source": [
    "#### __Reading a JSON File into a DataFrame in PySpark__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f9699c-78d6-41ef-9e6b-c32d3c38d2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elicer/workload'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6f3667-686b-4f72-b012-8a2742566658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 05:13:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              chosen|       conversations|            rejected|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{gpt, That sounds...|[{human, Hi! I'd ...|{gpt, Hello! I'd ...|\n",
      "|{gpt, In this tas...|[{system, You are...|{gpt, Sure, I'd b...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JSON File Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema Definition\n",
    "schema = StructType([\n",
    "    StructField(\"conversations\", ArrayType(StructType([\n",
    "        StructField(\"from\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"chosen\", StructType([\n",
    "        StructField(\"from\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"rejected\", StructType([\n",
    "        StructField(\"from\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# JRead a JSON file into a DataFrame and apply a schema\n",
    "file_path = \"/home/elicer/workload/file.json\"\n",
    "df = spark.read.option(\"multiLine\", True).json(file_path)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# If there are incorrect records, print them\n",
    "if \"_corrupt_record\" in df.columns:\n",
    "    corrupt_records = df.filter(df[\"_corrupt_record\"].isNotNull())\n",
    "    if corrupt_records.count() > 0:\n",
    "        print(\"Corrupt records found:\")\n",
    "        corrupt_records.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e7c7d-4cc3-4259-bae3-4ca0174f0d06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7f66c-8c57-4d05-9e1b-229121ff71d3",
   "metadata": {},
   "source": [
    "# Introduction to Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6429ce-f8be-4a3a-9d7c-87b83a91f6ca",
   "metadata": {},
   "source": [
    "TIP : The Apache Iceberg version is 1.5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208082e-d42f-4585-b3bc-65bbe90cd158",
   "metadata": {},
   "source": [
    "#### [What is Apache Iceberg?](https://iceberg.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2cfc3-b55d-4535-9212-1530a19ef4aa",
   "metadata": {},
   "source": [
    "- Apache Iceberg is an open-source table format and management library that is compatible with Apache Spark.\n",
    "- Provides the ability to efficiently manage and query structured data from data lakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce0053-1046-4329-917d-03c83ba21bd7",
   "metadata": {},
   "source": [
    "![스크린샷](iceberg1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52705d-8de0-4c76-9bfd-21022b1e4b50",
   "metadata": {},
   "source": [
    "![스크린샷](iceberg4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993451b-656f-408d-aac2-da98e7d8ba60",
   "metadata": {},
   "source": [
    "![스크린샷](iceberg3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598afd39-707f-42c8-acbf-ddef5c6d6248",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92afd0-3b2d-455c-b54e-eb820b2933d2",
   "metadata": {},
   "source": [
    "#### Contents \n",
    "- Load to Iceberg Table\n",
    "- Check Iceberg Table Schema\n",
    "- Time travel\n",
    "- Iceberg Table Writing\n",
    "- Iceberg Table Reading\n",
    "- Look up snapshot list\n",
    "- Spark Data Management and Query Example with Iceberg\n",
    "- Explanation of Rollback Functionality\n",
    "- Iceberg Schema Evolution\n",
    "- Update Table\n",
    "- Average, Aggregate, and Group\n",
    "- Verifying Specific Conditions Data\n",
    "- Query bulk data query\n",
    "- Delete Table (ID 2, delete the table)\n",
    "- Iceberg table integration and JSON data processing with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958ac4c-2135-49d4-9cd8-25e36dd4d377",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38578a1f-e030-43de-82ef-1f58a0b8cb16",
   "metadata": {},
   "source": [
    "#### __Load to Iceberg Table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba959b3d-fe0c-467e-8076-34d78f2de2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 05:19:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "|show_id|   type|               title|            director|                cast|             country|        date_added|release_year|rating| duration|           listed_in|         description|\n",
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "|     s1|  Movie|Dick Johnson Is Dead|     Kirsten Johnson|                NULL|       United States|September 25, 2021|        2020| PG-13|   90 min|       Documentaries|As her father nea...|\n",
      "|     s2|TV Show|       Blood & Water|                NULL|Ama Qamata, Khosi...|        South Africa|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|After crossing pa...|\n",
      "|     s3|TV Show|           Ganglands|     Julien Leclercq|Sami Bouajila, Tr...|                NULL|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|To protect his fa...|\n",
      "|     s4|TV Show|Jailbirds New Orl...|                NULL|                NULL|                NULL|September 24, 2021|        2021| TV-MA| 1 Season|Docuseries, Reali...|Feuds, flirtation...|\n",
      "|     s5|TV Show|        Kota Factory|                NULL|Mayur More, Jiten...|               India|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|In a city of coac...|\n",
      "|     s6|TV Show|       Midnight Mass|       Mike Flanagan|Kate Siegel, Zach...|                NULL|September 24, 2021|        2021| TV-MA| 1 Season|TV Dramas, TV Hor...|The arrival of a ...|\n",
      "|     s7|  Movie|My Little Pony: A...|Robert Cullen, Jo...|Vanessa Hudgens, ...|                NULL|September 24, 2021|        2021|    PG|   91 min|Children & Family...|Equestria's divid...|\n",
      "|     s8|  Movie|             Sankofa|        Haile Gerima|Kofi Ghanaba, Oya...|United States, Gh...|September 24, 2021|        1993| TV-MA|  125 min|Dramas, Independe...|On a photo shoot ...|\n",
      "|     s9|TV Show|The Great British...|     Andy Devonshire|Mel Giedroyc, Sue...|      United Kingdom|September 24, 2021|        2021| TV-14|9 Seasons|British TV Shows,...|A talented batch ...|\n",
      "|    s10|  Movie|        The Starling|      Theodore Melfi|Melissa McCarthy,...|       United States|September 24, 2021|        2021| PG-13|  104 min|    Comedies, Dramas|A woman adjusting...|\n",
      "|    s11|TV Show|Vendetta: Truth, ...|                NULL|                NULL|                NULL|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, D...|\"Sicily boasts a ...|\n",
      "|    s12|TV Show|    Bangkok Breaking|   Kongkiat Komesiri|Sukollawat Kanaro...|                NULL|September 23, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|Struggling to ear...|\n",
      "|    s13|  Movie|        Je Suis Karl| Christian Schwochow|Luna Wedler, Jann...|Germany, Czech Re...|September 23, 2021|        2021| TV-MA|  127 min|Dramas, Internati...|After most of her...|\n",
      "|    s14|  Movie|Confessions of an...|       Bruno Garotti|Klara Castanho, L...|                NULL|September 22, 2021|        2021| TV-PG|   91 min|Children & Family...|When the clever b...|\n",
      "|    s15|TV Show|Crime Stories: In...|                NULL|                NULL|                NULL|September 22, 2021|        2021| TV-MA| 1 Season|British TV Shows,...|Cameras following...|\n",
      "|    s16|TV Show|   Dear White People|                NULL|Logan Browning, B...|       United States|September 22, 2021|        2021| TV-MA|4 Seasons|TV Comedies, TV D...|\"Students of colo...|\n",
      "|    s17|  Movie|Europe's Most Dan...|Pedro de Echave G...|                NULL|                NULL|September 22, 2021|        2020| TV-MA|   67 min|Documentaries, In...|Declassified docu...|\n",
      "|    s18|TV Show|     Falsa identidad|                NULL|Luis Ernesto Fran...|              Mexico|September 22, 2021|        2020| TV-MA|2 Seasons|Crime TV Shows, S...|Strangers Diego a...|\n",
      "|    s19|  Movie|           Intrusion|          Adam Salky|Freida Pinto, Log...|                NULL|September 22, 2021|        2021| TV-14|   94 min|           Thrillers|After a deadly ho...|\n",
      "|    s20|TV Show|              Jaguar|                NULL|Blanca Suárez, Iv...|                NULL|September 22, 2021|        2021| TV-MA| 1 Season|International TV ...|In the 1960s, a H...|\n",
      "+-------+-------+--------------------+--------------------+--------------------+--------------------+------------------+------------+------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Setting and Authentication Key\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV files stored on the data hub\n",
    "csv_path = \"/mnt/elice/datahub/iceberg-poc/iceberg-tables/default/netflix_titles 2.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "\n",
    "# Iceberg Table Name\n",
    "table_name = \"netflix_titles\"\n",
    "\n",
    "# Load to Iceberg Table\n",
    "df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(f\"spark_catalog.default.{table_name}\")\n",
    "\n",
    "# Query data from the Iceberg table\n",
    "spark.table(f\"spark_catalog.default.{table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19edc8a-ebb1-48af-bdb8-292e72cdf198",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6c8bf-c756-4dab-a5eb-298cc1252830",
   "metadata": {},
   "source": [
    "#### __Check Iceberg Table Schema__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2451542-b374-4188-987c-1e8708520b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 05:21:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- cast: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Setting and Authentication Key\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Name of the Iceberg table\n",
    "table_name = \"netflix_titles\"\n",
    "\n",
    "# Schema lookup for an Iceberg table\n",
    "df = spark.table(f\"spark_catalog.default.{table_name}\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d096810-5171-4f32-b471-d91f16c48e29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070b800-a208-480c-9144-c81864f0ece9",
   "metadata": {},
   "source": [
    "#### __Iceberg Time travel__ \n",
    "Spark 3.3 and later supports time travel in SQL queries using TIMESTAMP AS OF or VERSION AS OF clauses. The VERSION AS OF clause can contain a long snapshot ID or a string branch or tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed212db0-9cc6-4f5d-a5e6-8cecfba911b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- time travel to October 26, 2024 at 12:21:00\n",
    "SELECT * FROM prod.db.table TIMESTAMP AS OF '2024-10-26 12:21:00';\n",
    "\n",
    "-- time travel to snapshot with id 10963874102873L\n",
    "SELECT * FROM prod.db.table VERSION AS OF 10963874102873;\n",
    "\n",
    "-- time travel to the head snapshot of ai-helpy\n",
    "SELECT * FROM prod.db.table VERSION AS OF 'ai-helpy';\n",
    "\n",
    "-- time travel to the snapshot referenced by the tag ai-snapshot\n",
    "SELECT * FROM prod.db.table VERSION AS OF 'ai-snapshot';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4116ee0-f735-4f1c-b98a-92cd7bf1bdb8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e131bde-210d-4be4-a29d-dda3b2e174ea",
   "metadata": {},
   "source": [
    "#### __Iceberg Table Writing__ \n",
    "Once your table is created, insert data using INSERT INTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62f8ef-94b6-43ed-8d07-b942babc7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT INTO local.db.table VALUES (1, 'a'), (2, 'b'), (3, 'c');\n",
    "INSERT INTO local.db.table SELECT id, data FROM source WHERE length(data) = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071dc7e9-3ed7-4e7e-ae2e-7845a41467ac",
   "metadata": {},
   "source": [
    "Iceberg also adds row-level SQL updates to Spark, MERGE INTO and DELETE FROM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4bb00-cacb-467f-bf39-256acb32e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE INTO local.db.target t USING (SELECT * FROM updates) u ON t.id = u.id\n",
    "WHEN MATCHED THEN UPDATE SET t.count = t.count + u.count\n",
    "WHEN NOT MATCHED THEN INSERT *;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dda40-6163-45e3-949b-ed762637434c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066f471-189d-427a-9cef-75ac7c8dd789",
   "metadata": {},
   "source": [
    "#### __Iceberg Table Reading__\n",
    "To read with SQL, use the Iceberg table's name in a SELECT query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eaa47b-c577-4a51-ae67-c0b066f89f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT count(1) as count, data\n",
    "FROM local.db.table\n",
    "GROUP BY data;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a53a1c-54cb-472f-bd0b-9faada62d401",
   "metadata": {},
   "source": [
    "SQL is also the recommended way to inspect tables. To view all snapshots in a table, use the snapshots metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132f2fa-3741-420b-964e-4c083c43656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM local.db.table.snapshots;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b034bd3-f781-4171-a723-a30290eb9f74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebe9f4-307a-40ea-aac5-24848ca7ec03",
   "metadata": {},
   "source": [
    "#### __Look up snapshot list__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc0a1ae-84c9-4475-afc9-3f9c406f66cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 04:16:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+---------+------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id|operation|manifest_list                                                                                                                       |summary                                                                                                                                                                                                                                                                                         |\n",
      "+-----------------------+-------------------+---------+---------+------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2024-07-10 03:07:36.823|6046816094557856117|NULL     |append   |s3a://iceberg-poc-4l0g37aq/iceberg-tables/default/test/metadata/snap-6046816094557856117-1-b9f38618-10a2-42f6-aa9b-17fa723e20a8.avro|{spark.app.id -> local-1720349417147, added-data-files -> 3, added-records -> 3, added-files-size -> 1878, changed-partition-count -> 1, total-records -> 3, total-files-size -> 1878, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "+-----------------------+-------------------+---------+---------+------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Look up snapshot list\n",
    "snapshot_list_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM spark_catalog.default.test.snapshots\n",
    "\"\"\"\n",
    "snapshot_list = spark.sql(snapshot_list_query)\n",
    "snapshot_list.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84e06c-be67-45a2-afbe-60a03909090a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51f1df-1358-4647-b649-a784b00f84ff",
   "metadata": {},
   "source": [
    "#### __Look up specific versions of the Iceberg table (snapshot)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f2e72a-6fb1-450d-9ec8-50f42b58a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 15:22:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/07/15 15:22:35 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|id |data|\n",
      "+---+----+\n",
      "|1  |a   |\n",
      "|2  |b   |\n",
      "|3  |c   |\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# snapshot_id : 6046816094557856117\n",
    "snapshot_list_query = \"\"\"\n",
    "    SELECT * FROM spark_catalog.default.test VERSION AS OF 6046816094557856117;\n",
    "\"\"\"\n",
    "snapshot_list = spark.sql(snapshot_list_query)\n",
    "snapshot_list.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd468b-af62-42e5-a45d-aa755d9cb36a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe96f9-1e2e-4896-bfe8-5bb14cf608d1",
   "metadata": {},
   "source": [
    "#### __How to use snapshots for time travel__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593e50fb-be2d-40ea-83d6-a887d23dbf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 15:33:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|id |data|\n",
      "+---+----+\n",
      "|1  |a   |\n",
      "|2  |b   |\n",
      "|3  |c   |\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# Required JAR File Path\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS S3 Access Key Settings\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "\n",
    "# Iceberg Metadata Location\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Date and time string for time travel\n",
    "committed_at_string = \"2024-07-10 03:07:36.823\"\n",
    "\n",
    "# Converting date and time strings to Timestamp objects\n",
    "committed_at = datetime.strptime(committed_at_string, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "# SQL query for time travel in the Iceberg table\n",
    "snapshot_query = f\"\"\"\n",
    "    SELECT * FROM spark_catalog.default.test TIMESTAMP AS OF TIMESTAMP '{committed_at}'\n",
    "\"\"\"\n",
    "\n",
    "# Time Travel Run\n",
    "snapshot_df = spark.sql(snapshot_query)\n",
    "\n",
    "# Output Results\n",
    "snapshot_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58102708-b132-41c8-a178-b200f5d4098a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38835f4-3427-4112-b47f-2cae6858fd59",
   "metadata": {},
   "source": [
    "#### __Spark Data Management and Query Example with Iceberg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a69698-7a4a-46c6-89cf-fdc6eace4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 16:43:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n",
      "Tables in Iceberg Catalog:\n",
      "Row(namespace='default', tableName='iceberg_table', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample7', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample6', isTemporary=False)\n",
      "Row(namespace='default', tableName='netflix_titles', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample3', isTemporary=False)\n",
      "Row(namespace='default', tableName='users', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample2', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample11', isTemporary=False)\n",
      "Row(namespace='default', tableName='test_tables', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample_schema', isTemporary=False)\n",
      "Row(namespace='default', tableName='sample9', isTemporary=False)\n",
      "Row(namespace='default', tableName='test', isTemporary=False)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set JAR file paths\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS S3 credentials and Iceberg metadata location\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define function to query Iceberg table\n",
    "def query_iceberg_table(table_name):\n",
    "    return spark.sql(f\"SELECT * FROM spark_catalog.default.{table_name}\")\n",
    "\n",
    "# Example usage: Query the 'sample' table\n",
    "sample_df = query_iceberg_table(\"sample\")\n",
    "sample_df.show()\n",
    "\n",
    "# Define function to drop Iceberg table\n",
    "def drop_iceberg_table(table_name):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS spark_catalog.default.{table_name}\")\n",
    "\n",
    "# Example usage: Drop the 'sample' table\n",
    "drop_iceberg_table(\"sample\")\n",
    "\n",
    "# Define function to list all tables in Iceberg catalog\n",
    "def list_iceberg_tables():\n",
    "    return spark.sql(\"SHOW TABLES IN spark_catalog.default\").collect()\n",
    "\n",
    "# Example usage: List all tables in the Iceberg catalog\n",
    "tables = list_iceberg_tables()\n",
    "print(\"Tables in Iceberg Catalog:\")\n",
    "for table in tables:\n",
    "    print(table)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a97354-57ad-4b1e-bd8c-bfa74a183b02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad191c8-4c95-4d8e-a842-d6f269f50ee1",
   "metadata": {},
   "source": [
    "#### __Explanation of Rollback Functionality__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b04dd84-c2bc-41da-be0a-775e27dc658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 17:02:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  4|   d|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "|  5|   e|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define your configuration and jar paths\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# Define S3 credentials and Iceberg metadata location\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Initialize SparkSession with Iceberg integration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create table if not exists and insert initial data\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark_catalog.default.sample (id bigint, data string) USING iceberg\")\n",
    "spark.sql(\"INSERT INTO spark_catalog.default.sample VALUES (1, 'a'), (2, 'b'), (3, 'c')\")\n",
    "\n",
    "# Capture the initial snapshot ID\n",
    "initial_snapshot = spark.sql(\"SELECT snapshot_id FROM spark_catalog.default.sample.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0][0]\n",
    "\n",
    "try:\n",
    "    # Perform data operations\n",
    "    spark.sql(\"INSERT INTO spark_catalog.default.sample VALUES (4, 'd'), (5, 'e')\")\n",
    "    print(\"Data inserted successfully.\")\n",
    "except Exception as e:\n",
    "    # Rollback to the initial snapshot in case of an error\n",
    "    spark.sql(f\"CALL spark_catalog.system.rollback_to_snapshot('default.sample', {initial_snapshot})\")\n",
    "    print(f\"Transaction rolled back due to error: {str(e)}\")\n",
    "finally:\n",
    "    # Query the table to verify the data\n",
    "    result = spark.sql(\"SELECT * FROM spark_catalog.default.sample\")\n",
    "    result.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd61a2-9b1f-4447-b34f-8e1e298681ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9dcbff-8d3f-4560-94ae-8afc85665d35",
   "metadata": {},
   "source": [
    "#### __Iceberg Schema Evolution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8c26e5f-eecd-4ad5-8bf9-e170de506e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 16:19:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|data|new_column|\n",
      "+---+----+----------+\n",
      "|  1|   a|       foo|\n",
      "|  2|   b|       bar|\n",
      "|  3|   c|       baz|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Paths to necessary JAR files\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# S3 connection details\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergSchemaManagement\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Iceberg table (initial schema)\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark_catalog.default.sample (id bigint, data string) USING iceberg\")\n",
    "\n",
    "# Alter Iceberg table schema (update with a new column)\n",
    "spark.sql(\"ALTER TABLE spark_catalog.default.sample ADD COLUMN new_column string\")\n",
    "\n",
    "# Insert data\n",
    "spark.sql(\"INSERT INTO spark_catalog.default.sample VALUES (1, 'a', 'foo'), (2, 'b', 'bar'), (3, 'c', 'baz')\")\n",
    "\n",
    "# Query data\n",
    "result = spark.sql(\"SELECT * FROM spark_catalog.default.sample\")\n",
    "result.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b61a2-8c85-4ec0-a542-7ae28d50afad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79b5ea-10ac-4814-b0a2-1a7f5013e0d5",
   "metadata": {},
   "source": [
    "#### __Create Table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3386d29-5751-44ed-a74e-6a7f1a72c7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 04:51:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Paths to the required JAR files for Hadoop, AWS SDK, and Iceberg\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS credentials and Iceberg metadata location on S3\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create a Spark session with configurations for Iceberg and S3 integration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a table named 'sample' in the Iceberg catalog if it does not already exist\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS spark_catalog.default.sample (id bigint, data string) USING iceberg\")\n",
    "\n",
    "# Insert some sample data into the 'sample' table\n",
    "spark.sql(\"INSERT INTO spark_catalog.default.sample VALUES (1, 'a'), (2, 'b'), (3, 'c')\")\n",
    "\n",
    "# Query the 'sample' table and display the results\n",
    "result = spark.sql(\"SELECT * FROM spark_catalog.default.sample\")\n",
    "result.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1135d81-3163-409c-b788-84939fc10d9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65edd91d-a590-4816-a50b-19023c447055",
   "metadata": {},
   "source": [
    "#### __Update Table__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c01936-f090-4b96-8986-9066f62edf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 04:19:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table creation took 3.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion took 3.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+-----+--------------+-------------------+\n",
      "| id|backend_id|    function_api_url| type|          text|           log_time|\n",
      "+---+----------+--------------------+-----+--------------+-------------------+\n",
      "|  1|  backend1|http://api.exampl...|type1| Sample text 1|2024-07-03 10:00:00|\n",
      "|  2|  backend2|http://api.exampl...|type2| Sample text 2|2024-07-03 11:00:00|\n",
      "|  3|  backend3|http://api.exampl...|type3| Sample text 3|2024-07-03 12:00:00|\n",
      "|  4|  backend1|http://api.exampl...|type1| Sample text 4|2024-07-03 13:00:00|\n",
      "|  5|  backend2|http://api.exampl...|type2| Sample text 5|2024-07-03 14:00:00|\n",
      "|  6|  backend3|http://api.exampl...|type3| Sample text 6|2024-07-03 15:00:00|\n",
      "|  7|  backend1|http://api.exampl...|type1| Sample text 7|2024-07-03 16:00:00|\n",
      "|  8|  backend2|http://api.exampl...|type2| Sample text 8|2024-07-03 17:00:00|\n",
      "|  9|  backend3|http://api.exampl...|type3| Sample text 9|2024-07-03 18:00:00|\n",
      "| 10|  backend1|http://api.exampl...|type1|Sample text 10|2024-07-03 19:00:00|\n",
      "| 11|  backend2|http://api.exampl...|type2|Sample text 11|2024-07-03 20:00:00|\n",
      "| 12|  backend3|http://api.exampl...|type3|Sample text 12|2024-07-03 21:00:00|\n",
      "| 13|  backend1|http://api.exampl...|type1|Sample text 13|2024-07-03 22:00:00|\n",
      "| 14|  backend2|http://api.exampl...|type2|Sample text 14|2024-07-03 23:00:00|\n",
      "| 15|  backend3|http://api.exampl...|type3|Sample text 15|2024-07-04 00:00:00|\n",
      "| 16|  backend1|http://api.exampl...|type1|Sample text 16|2024-07-04 01:00:00|\n",
      "| 17|  backend2|http://api.exampl...|type2|Sample text 17|2024-07-04 02:00:00|\n",
      "| 18|  backend3|http://api.exampl...|type3|Sample text 18|2024-07-04 03:00:00|\n",
      "| 19|  backend1|http://api.exampl...|type1|Sample text 19|2024-07-04 04:00:00|\n",
      "| 20|  backend2|http://api.exampl...|type2|Sample text 20|2024-07-04 05:00:00|\n",
      "+---+----------+--------------------+-----+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Data update took 4.95 seconds\n",
      "+---+----------+--------------------+-----+------------+-------------------+\n",
      "| id|backend_id|    function_api_url| type|        text|           log_time|\n",
      "+---+----------+--------------------+-----+------------+-------------------+\n",
      "|  1|  backend1|http://api.exampl...|type1|Updated text|2024-07-03 10:00:00|\n",
      "+---+----------+--------------------+-----+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Paths to JAR files required for Hadoop, AWS SDK, and Iceberg\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS credentials and Iceberg metadata location on S3\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create a Spark session with configurations for Iceberg and S3 integration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SQL query to create a table named 'test_tables' in the Iceberg catalog if it does not already exist\n",
    "create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_catalog.default.test_tables (\n",
    "        id INTEGER,\n",
    "        backend_id VARCHAR(255),\n",
    "        function_api_url VARCHAR(255),\n",
    "        type VARCHAR(50),\n",
    "        text STRING,\n",
    "        log_time TIMESTAMP\n",
    "    ) USING iceberg\n",
    "\"\"\"\n",
    "\n",
    "# Measure time taken to create the table\n",
    "start_time_create = time.time()\n",
    "spark.sql(create_table_query)\n",
    "end_time_create = time.time()\n",
    "create_duration = end_time_create - start_time_create\n",
    "print(f\"Table creation took {create_duration:.2f} seconds\")\n",
    "\n",
    "# SQL query to insert multiple rows of data into the 'test_tables' table\n",
    "insert_data_query = \"\"\"\n",
    "    INSERT INTO spark_catalog.default.test_tables\n",
    "    VALUES \n",
    "        (1, 'backend1', 'http://api.example.com/1', 'type1', 'Sample text 1', TIMESTAMP '2024-07-03 10:00:00'),\n",
    "        (2, 'backend2', 'http://api.example.com/2', 'type2', 'Sample text 2', TIMESTAMP '2024-07-03 11:00:00'),\n",
    "        (3, 'backend3', 'http://api.example.com/3', 'type3', 'Sample text 3', TIMESTAMP '2024-07-03 12:00:00'),\n",
    "        ...\n",
    "        (20, 'backend2', 'http://api.example.com/2', 'type2', 'Sample text 20', TIMESTAMP '2024-07-04 05:00:00')\n",
    "\"\"\"\n",
    "\n",
    "# Measure time taken to insert the data\n",
    "start_time_insert = time.time()\n",
    "spark.sql(insert_data_query)\n",
    "end_time_insert = time.time()\n",
    "insert_duration = end_time_insert - start_time_insert\n",
    "print(f\"Data insertion took {insert_duration:.2f} seconds\")\n",
    "\n",
    "# Query and display all records from the 'test_tables' table\n",
    "result = spark.sql(\"SELECT * FROM spark_catalog.default.test_tables\")\n",
    "result.show()\n",
    "\n",
    "# SQL query to update the 'text' field of the record with id = 1\n",
    "update_data_query = \"\"\"\n",
    "    UPDATE spark_catalog.default.test_tables\n",
    "    SET text = 'Updated text'\n",
    "    WHERE id = 1\n",
    "\"\"\"\n",
    "\n",
    "# Measure time taken to update the data\n",
    "start_time_update = time.time()\n",
    "spark.sql(update_data_query)\n",
    "end_time_update = time.time()\n",
    "update_duration = end_time_update - start_time_update\n",
    "print(f\"Data update took {update_duration:.2f} seconds\")\n",
    "\n",
    "# Query and display the updated record with id = 1\n",
    "updated_result = spark.sql(\"SELECT * FROM spark_catalog.default.test_tables WHERE id = 1\")\n",
    "updated_result.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354cea8-8512-48f5-afb4-c2b4d69f60d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d2b53-0def-44dd-b176-1edf0c979e98",
   "metadata": {},
   "source": [
    "#### __Average, Aggregate, and Group__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5363c099-8ce4-469b-8fd6-e14f1e04a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 04:26:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+\n",
      "| type|count(1)|avg(id)|\n",
      "+-----+--------+-------+\n",
      "|type3|      10|   16.5|\n",
      "|type1|      10|   14.5|\n",
      "|type2|      10|   15.5|\n",
      "+-----+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Average, Aggregate, and Group\n",
    "complex_query = \"\"\"\n",
    "    SELECT type, COUNT(*), AVG(id)\n",
    "    FROM spark_catalog.default.test_tables\n",
    "    GROUP BY type\n",
    "\"\"\"\n",
    "\n",
    "complex_result = spark.sql(complex_query)\n",
    "complex_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c4bef-2b7a-47c0-9758-0cd668721a0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f1574-28db-4839-baf9-b9a5b0c972b2",
   "metadata": {},
   "source": [
    "#### __Verifying Specific Conditions Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017e84e3-ed3f-4f51-b85e-1923f8bd316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 04:27:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema alteration took 4.04 seconds\n",
      "+----------------+---------+-------+\n",
      "|        col_name|data_type|comment|\n",
      "+----------------+---------+-------+\n",
      "|              id|      int|   NULL|\n",
      "|      backend_id|   string|   NULL|\n",
      "|function_api_url|   string|   NULL|\n",
      "|            type|   string|   NULL|\n",
      "|            text|   string|   NULL|\n",
      "|        log_time|timestamp|   NULL|\n",
      "|      new_column|   string|   NULL|\n",
      "+----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Paths to JAR files required for Hadoop, AWS SDK, and Iceberg\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS credentials and Iceberg metadata location on S3\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create a Spark session with configurations for Iceberg and S3 integration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SQL query to alter the table by adding a new column named 'new_column' of type STRING\n",
    "alter_table_query = \"\"\"\n",
    "    ALTER TABLE spark_catalog.default.test_tables\n",
    "    ADD COLUMNS (new_column STRING)\n",
    "\"\"\"\n",
    "\n",
    "# Measure time taken to execute the schema alteration\n",
    "start_time_alter = time.time()\n",
    "spark.sql(alter_table_query)\n",
    "end_time_alter = time.time()\n",
    "alter_duration = end_time_alter - start_time_alter\n",
    "print(f\"Schema alteration took {alter_duration:.2f} seconds\")\n",
    "\n",
    "# Query to describe the table schema and display the result\n",
    "altered_result = spark.sql(\"DESCRIBE spark_catalog.default.test_tables\")\n",
    "altered_result.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ee280-b751-4331-8d90-7af685e80dc4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7546b91-373b-459d-be5a-7f40b0bb78e3",
   "metadata": {},
   "source": [
    "#### __Query bulk data query__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09bc53-4262-451c-b3c3-bec4035c98b5",
   "metadata": {},
   "source": [
    "performance_result.show (n=1000, truncate=False) is a command that outputs data processed by Spark to the screen without truncating the entire content, including up to 1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8091a149-1e03-488c-8404-e8548f3dffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 04:30:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------------+-----+--------------+-------------------+----------+\n",
      "|id |backend_id|function_api_url        |type |text          |log_time           |new_column|\n",
      "+---+----------+------------------------+-----+--------------+-------------------+----------+\n",
      "|30 |backend3  |http://api.example.com/3|type3|Sample text 30|2024-07-04 15:00:00|NULL      |\n",
      "|29 |backend2  |http://api.example.com/2|type2|Sample text 29|2024-07-04 14:00:00|NULL      |\n",
      "|28 |backend1  |http://api.example.com/1|type1|Sample text 28|2024-07-04 13:00:00|NULL      |\n",
      "|27 |backend3  |http://api.example.com/3|type3|Sample text 27|2024-07-04 12:00:00|NULL      |\n",
      "|26 |backend2  |http://api.example.com/2|type2|Sample text 26|2024-07-04 11:00:00|NULL      |\n",
      "|25 |backend1  |http://api.example.com/1|type1|Sample text 25|2024-07-04 10:00:00|NULL      |\n",
      "|24 |backend3  |http://api.example.com/3|type3|Sample text 24|2024-07-04 09:00:00|NULL      |\n",
      "|23 |backend2  |http://api.example.com/2|type2|Sample text 23|2024-07-04 08:00:00|NULL      |\n",
      "|22 |backend1  |http://api.example.com/1|type1|Sample text 22|2024-07-04 07:00:00|NULL      |\n",
      "|21 |backend3  |http://api.example.com/3|type3|Sample text 21|2024-07-04 06:00:00|NULL      |\n",
      "|20 |backend2  |http://api.example.com/2|type2|Sample text 20|2024-07-04 05:00:00|NULL      |\n",
      "|19 |backend1  |http://api.example.com/1|type1|Sample text 19|2024-07-04 04:00:00|NULL      |\n",
      "|18 |backend3  |http://api.example.com/3|type3|Sample text 18|2024-07-04 03:00:00|NULL      |\n",
      "|17 |backend2  |http://api.example.com/2|type2|Sample text 17|2024-07-04 02:00:00|NULL      |\n",
      "|16 |backend1  |http://api.example.com/1|type1|Sample text 16|2024-07-04 01:00:00|NULL      |\n",
      "|15 |backend3  |http://api.example.com/3|type3|Sample text 15|2024-07-04 00:00:00|NULL      |\n",
      "|14 |backend2  |http://api.example.com/2|type2|Sample text 14|2024-07-03 23:00:00|NULL      |\n",
      "|13 |backend1  |http://api.example.com/1|type1|Sample text 13|2024-07-03 22:00:00|NULL      |\n",
      "|12 |backend3  |http://api.example.com/3|type3|Sample text 12|2024-07-03 21:00:00|NULL      |\n",
      "|11 |backend2  |http://api.example.com/2|type2|Sample text 11|2024-07-03 20:00:00|NULL      |\n",
      "|10 |backend1  |http://api.example.com/1|type1|Sample text 10|2024-07-03 19:00:00|NULL      |\n",
      "|9  |backend3  |http://api.example.com/3|type3|Sample text 9 |2024-07-03 18:00:00|NULL      |\n",
      "|8  |backend2  |http://api.example.com/2|type2|Sample text 8 |2024-07-03 17:00:00|NULL      |\n",
      "|7  |backend1  |http://api.example.com/1|type1|Sample text 7 |2024-07-03 16:00:00|NULL      |\n",
      "|6  |backend3  |http://api.example.com/3|type3|Sample text 6 |2024-07-03 15:00:00|NULL      |\n",
      "|5  |backend2  |http://api.example.com/2|type2|Sample text 5 |2024-07-03 14:00:00|NULL      |\n",
      "|4  |backend1  |http://api.example.com/1|type1|Sample text 4 |2024-07-03 13:00:00|NULL      |\n",
      "|3  |backend3  |http://api.example.com/3|type3|Sample text 3 |2024-07-03 12:00:00|NULL      |\n",
      "|2  |backend2  |http://api.example.com/2|type2|Sample text 2 |2024-07-03 11:00:00|NULL      |\n",
      "|1  |backend1  |http://api.example.com/1|type1|Updated text  |2024-07-03 10:00:00|NULL      |\n",
      "+---+----------+------------------------+-----+--------------+-------------------+----------+\n",
      "\n",
      "Query execution took 1.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Paths to JAR files required for Hadoop, AWS SDK, and Iceberg\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS credentials and Iceberg metadata location on S3\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create a Spark session with configurations for Iceberg and S3 integration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Measure the performance of a query that retrieves and orders a large amount of data\n",
    "start_time_performance = time.time()\n",
    "\n",
    "# Define the SQL query to select the top 1000 rows from 'test_tables', ordered by 'log_time' in descending order\n",
    "performance_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM spark_catalog.default.test_tables\n",
    "    ORDER BY log_time DESC\n",
    "    LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and fetch the results\n",
    "performance_result = spark.sql(performance_query)\n",
    "\n",
    "# Display the top 1000 rows of the result without truncating the output\n",
    "performance_result.show(n=1000, truncate=False)\n",
    "\n",
    "# Measure and print the duration of the query execution\n",
    "end_time_performance = time.time()\n",
    "performance_duration = end_time_performance - start_time_performance\n",
    "print(f\"Query execution took {performance_duration:.2f} seconds\")\n",
    "\n",
    "# Stop the Spark session to release resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446451b-b535-4415-b71a-62be77d94e24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee734c7-b202-453e-8efe-e120cfae6bb5",
   "metadata": {},
   "source": [
    "#### __Delete Table (ID 2, delete the table)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22dcf19-ac3b-48e4-85df-209abc61472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 05:40:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data deletion took 5.81 seconds\n",
      "+---+----------+------------------------+-----+--------------+-------------------+----------+\n",
      "|id |backend_id|function_api_url        |type |text          |log_time           |new_column|\n",
      "+---+----------+------------------------+-----+--------------+-------------------+----------+\n",
      "|1  |backend1  |http://api.example.com/1|type1|Updated text  |2024-07-03 10:00:00|NULL      |\n",
      "|3  |backend3  |http://api.example.com/3|type3|Sample text 3 |2024-07-03 12:00:00|NULL      |\n",
      "|4  |backend1  |http://api.example.com/1|type1|Sample text 4 |2024-07-03 13:00:00|NULL      |\n",
      "|5  |backend2  |http://api.example.com/2|type2|Sample text 5 |2024-07-03 14:00:00|NULL      |\n",
      "|6  |backend3  |http://api.example.com/3|type3|Sample text 6 |2024-07-03 15:00:00|NULL      |\n",
      "|7  |backend1  |http://api.example.com/1|type1|Sample text 7 |2024-07-03 16:00:00|NULL      |\n",
      "|8  |backend2  |http://api.example.com/2|type2|Sample text 8 |2024-07-03 17:00:00|NULL      |\n",
      "|9  |backend3  |http://api.example.com/3|type3|Sample text 9 |2024-07-03 18:00:00|NULL      |\n",
      "|10 |backend1  |http://api.example.com/1|type1|Sample text 10|2024-07-03 19:00:00|NULL      |\n",
      "|11 |backend2  |http://api.example.com/2|type2|Sample text 11|2024-07-03 20:00:00|NULL      |\n",
      "|12 |backend3  |http://api.example.com/3|type3|Sample text 12|2024-07-03 21:00:00|NULL      |\n",
      "|13 |backend1  |http://api.example.com/1|type1|Sample text 13|2024-07-03 22:00:00|NULL      |\n",
      "|14 |backend2  |http://api.example.com/2|type2|Sample text 14|2024-07-03 23:00:00|NULL      |\n",
      "|15 |backend3  |http://api.example.com/3|type3|Sample text 15|2024-07-04 00:00:00|NULL      |\n",
      "|16 |backend1  |http://api.example.com/1|type1|Sample text 16|2024-07-04 01:00:00|NULL      |\n",
      "|17 |backend2  |http://api.example.com/2|type2|Sample text 17|2024-07-04 02:00:00|NULL      |\n",
      "|18 |backend3  |http://api.example.com/3|type3|Sample text 18|2024-07-04 03:00:00|NULL      |\n",
      "|19 |backend1  |http://api.example.com/1|type1|Sample text 19|2024-07-04 04:00:00|NULL      |\n",
      "|20 |backend2  |http://api.example.com/2|type2|Sample text 20|2024-07-04 05:00:00|NULL      |\n",
      "|21 |backend3  |http://api.example.com/3|type3|Sample text 21|2024-07-04 06:00:00|NULL      |\n",
      "+---+----------+------------------------+-----+--------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Paths to JAR files needed for Hadoop, AWS SDK, and Iceberg integration\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "# AWS credentials and location for Iceberg metadata on S3\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/iceberg-tables\"\n",
    "\n",
    "# Create a Spark session with configurations for Iceberg and S3\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SQL query to delete data from 'test_tables' where the id is 2\n",
    "delete_data_query = \"\"\"\n",
    "    DELETE FROM spark_catalog.default.test_tables\n",
    "    WHERE id = 2\n",
    "\"\"\"\n",
    "\n",
    "# Measure the time taken to execute the data deletion\n",
    "start_time_delete = time.time()\n",
    "\n",
    "# Execute the deletion query\n",
    "spark.sql(delete_data_query)\n",
    "\n",
    "end_time_delete = time.time()\n",
    "delete_duration = end_time_delete - start_time_delete\n",
    "print(f\"Data deletion took {delete_duration:.2f} seconds\")\n",
    "\n",
    "# Verify that the data has been deleted by trying to select rows with id = 2\n",
    "deleted_result = spark.sql(\"SELECT * FROM spark_catalog.default.test_tables WHERE id = 2\")\n",
    "deleted_result.show(truncate=False)\n",
    "\n",
    "# Fetch and display all rows from the table to confirm the current state\n",
    "result = spark.sql(\"SELECT * FROM spark_catalog.default.test_tables\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session to release resources\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83df348-5f6f-486c-b473-284f941294af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97303ce7-1f57-4c5e-900c-f64e2365d541",
   "metadata": {},
   "source": [
    "#### __Iceberg table integration and JSON data processing with PySpark__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb1ddd6-3b98-4ea2-a46f-430f51d34caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 05:46:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/08/01 05:46:46 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              chosen|       conversations|            rejected|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{gpt, That sounds...|[{human, Hi! I'd ...|{gpt, Hello! I'd ...|\n",
      "|{gpt, In this tas...|[{system, You are...|{gpt, Sure, I'd b...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Data saved to Iceberg table successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Paths and credentials\n",
    "hadoop_aws_jar = \"/home/elicer/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "aws_sdk_jar = \"/home/elicer/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "iceberg_spark_jar = \"/home/elicer/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar\"\n",
    "hadoop_common_jar = \"/home/elicer/spark/jars/hadoop-common-3.3.4.jar\"\n",
    "\n",
    "s3_access_key = \"Add your own access_key value\"\n",
    "s3_secret_key = \"Add your own secret_key value\"\n",
    "iceberg_metadata_location = \"s3a://iceberg-poc-4l0g37aq/helpy_table\"\n",
    "\n",
    "# Spark Session Build\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergIntegration\") \\\n",
    "    .config(\"spark.jars\", f\"{hadoop_aws_jar},{aws_sdk_jar},{iceberg_spark_jar},{hadoop_common_jar}\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_metadata_location) \\\n",
    "    .config(\"fs.s3a.access.key\", s3_access_key) \\\n",
    "    .config(\"fs.s3a.secret.key\", s3_secret_key) \\\n",
    "    .config(\"fs.s3a.endpoint\", \"https://datahub-central-01.elice.io\") \\\n",
    "    .config(\"fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Create Iceberg Table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS spark_catalog.default.helpy (\n",
    "        conversations ARRAY<STRUCT<from: STRING, value: STRING>>,\n",
    "        chosen STRUCT<from: STRING, value: STRING>,\n",
    "        rejected STRUCT<from: STRING, value: STRING>\n",
    "    ) \n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "    # Converting JSON data to Spark DataFrame\n",
    "    file_path = \"/home/elicer/workload/file.json\"\n",
    "    df = spark.read.option(\"multiLine\", True).json(file_path)\n",
    "\n",
    "    # Check if _corrupt_record column exists, and output a problematic record\n",
    "    if \"_corrupt_record\" in df.columns:\n",
    "        corrupt_records = df.filter(df[\"_corrupt_record\"].isNotNull())\n",
    "        if corrupt_records.count() > 0:\n",
    "            print(\"Corrupt records found:\")\n",
    "            corrupt_records.show(truncate=False)\n",
    "\n",
    "    # Store data in the Iceberg table\n",
    "    df.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"spark_catalog.default.helpy\")\n",
    "\n",
    "    df.show()\n",
    "    \n",
    "    print(\"Data saved to Iceberg table successfully.\")\n",
    "\n",
    "finally:\n",
    "    # Spark Session Ending\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca2df6a4-b02c-442a-9858-0727f5e7bcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmetadata\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/elice/datahub/iceberg-poc/helpy_table/default/helpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
